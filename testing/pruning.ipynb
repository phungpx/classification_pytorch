{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f89476-758f-431a-ac7e-41b375ffae5e",
   "metadata": {},
   "source": [
    "Will learn how to use `torch.nn.utils.prune` to sparsify your networks, and how to extend it to implement your own custom pruning technique.\n",
    "\n",
    "* Requirement: torch>=1.4.0a0+8e8a5e0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a2416-501e-4cc8-9c6d-d2164da21911",
   "metadata": {},
   "source": [
    "1. Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae6eefc4-ea17-4892-beed-ce4f115aa667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 60074\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet().to(device=device)\n",
    "\n",
    "print(f\"Number of parameters: {sum((p.numel() for p in model.parameters() if p.requires_grad))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221b1ee-8f6c-4a16-9ee4-f38b619d24cb",
   "metadata": {},
   "source": [
    "2. Inspect a Module\n",
    "\n",
    "Let's inspect the (unpruned) `conv1` layer in LetNet model. It will contain 2 parameters `weight` and `bias`, and no buffer, for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b29d113c-fbea-442d-b281-0424ea9575ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[[[ 0.2210,  0.2963, -0.1385],\n",
      "          [-0.2852,  0.1365, -0.1321],\n",
      "          [-0.2140, -0.2728,  0.3242]]],\n",
      "\n",
      "\n",
      "        [[[-0.2939,  0.2436, -0.2873],\n",
      "          [-0.2664, -0.1056, -0.1238],\n",
      "          [-0.3064,  0.2746, -0.1016]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1405,  0.1296, -0.2949],\n",
      "          [-0.2292,  0.0637, -0.3108],\n",
      "          [ 0.0451, -0.2222, -0.0419]]],\n",
      "\n",
      "\n",
      "        [[[-0.1592, -0.0800, -0.2572],\n",
      "          [-0.0362,  0.0274, -0.0213],\n",
      "          [ 0.0568,  0.0709,  0.3153]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2424,  0.1485, -0.2695],\n",
      "          [ 0.0359,  0.0299,  0.2123],\n",
      "          [-0.0808,  0.0516, -0.1402]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1840, -0.0456,  0.2920],\n",
      "          [-0.1896, -0.3016, -0.0747],\n",
      "          [-0.0215, -0.1728, -0.0849]]]], requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([ 0.1951,  0.1546, -0.1809,  0.2764, -0.2611, -0.2624],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "module = model.conv1\n",
    "print(list(module.named_parameters()))  # List[Tuple[name of parameters, tensor of parameters]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2120cc09-9240-4130-bdb7-be705020a7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))  # have no buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1caf3-9b09-4a48-be8a-44b623a09c27",
   "metadata": {},
   "source": [
    "3. Pruning a Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c922a960-584c-4e93-a268-fe29708b0dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da09308b-5bb8-4721-bb4a-b7c86cd2a31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([ 0.1951,  0.1546, -0.1809,  0.2764, -0.2611, -0.2624],\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[ 0.2210,  0.2963, -0.1385],\n",
      "          [-0.2852,  0.1365, -0.1321],\n",
      "          [-0.2140, -0.2728,  0.3242]]],\n",
      "\n",
      "\n",
      "        [[[-0.2939,  0.2436, -0.2873],\n",
      "          [-0.2664, -0.1056, -0.1238],\n",
      "          [-0.3064,  0.2746, -0.1016]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1405,  0.1296, -0.2949],\n",
      "          [-0.2292,  0.0637, -0.3108],\n",
      "          [ 0.0451, -0.2222, -0.0419]]],\n",
      "\n",
      "\n",
      "        [[[-0.1592, -0.0800, -0.2572],\n",
      "          [-0.0362,  0.0274, -0.0213],\n",
      "          [ 0.0568,  0.0709,  0.3153]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2424,  0.1485, -0.2695],\n",
      "          [ 0.0359,  0.0299,  0.2123],\n",
      "          [-0.0808,  0.0516, -0.1402]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1840, -0.0456,  0.2920],\n",
      "          [-0.1896, -0.3016, -0.0747],\n",
      "          [-0.0215, -0.1728, -0.0849]]]], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a46b3e-820a-4ca6-a4c4-4fa2bf950ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 1, 6, 0, 2, 4, 5, 7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "index = torch.randperm(batch_size)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59005b18-f156-4e27-a722-fe002162dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Anchors(nn.Module):\n",
    "    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n",
    "        super(Anchors, self).__init__()\n",
    "\n",
    "        if pyramid_levels is None:\n",
    "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
    "        if strides is None:\n",
    "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
    "        if sizes is None:\n",
    "            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n",
    "        if ratios is None:\n",
    "            self.ratios = np.array([0.5, 1, 2])\n",
    "        if scales is None:\n",
    "            self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        image_shape = image.shape[2:]\n",
    "        image_shape = np.array(image_shape)\n",
    "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
    "        print(image_shapes)\n",
    "\n",
    "        # compute anchors over all pyramid levels\n",
    "        all_anchors = np.zeros((0, 4)).astype(np.float32)\n",
    "\n",
    "        for idx, p in enumerate(self.pyramid_levels):\n",
    "            anchors         = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n",
    "            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n",
    "            all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
    "\n",
    "        all_anchors = np.expand_dims(all_anchors, axis=0)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.from_numpy(all_anchors.astype(np.float32)).cuda()\n",
    "        else:\n",
    "            return torch.from_numpy(all_anchors.astype(np.float32))\n",
    "\n",
    "def generate_anchors(base_size=16, ratios=None, scales=None):\n",
    "    \"\"\"\n",
    "    Generate anchor (reference) windows by enumerating aspect ratios X\n",
    "    scales w.r.t. a reference window.\n",
    "    \"\"\"\n",
    "\n",
    "    if ratios is None:\n",
    "        ratios = np.array([0.5, 1, 2])\n",
    "\n",
    "    if scales is None:\n",
    "        scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "    num_anchors = len(ratios) * len(scales)\n",
    "\n",
    "    # initialize output anchors\n",
    "    anchors = np.zeros((num_anchors, 4))\n",
    "\n",
    "    # scale base_size\n",
    "    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
    "\n",
    "    # compute areas of anchors\n",
    "    areas = anchors[:, 2] * anchors[:, 3]\n",
    "\n",
    "    # correct for ratios\n",
    "    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
    "    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
    "\n",
    "    # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n",
    "    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
    "    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n",
    "\n",
    "    return anchors\n",
    "\n",
    "def compute_shape(image_shape, pyramid_levels):\n",
    "    \"\"\"Compute shapes based on pyramid levels.\n",
    "    :param image_shape:\n",
    "    :param pyramid_levels:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    image_shape = np.array(image_shape[:2])\n",
    "    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]\n",
    "    return image_shapes\n",
    "\n",
    "\n",
    "def anchors_for_shape(\n",
    "    image_shape,\n",
    "    pyramid_levels=None,\n",
    "    ratios=None,\n",
    "    scales=None,\n",
    "    strides=None,\n",
    "    sizes=None,\n",
    "    shapes_callback=None,\n",
    "):\n",
    "\n",
    "    image_shapes = compute_shape(image_shape, pyramid_levels)\n",
    "\n",
    "    # compute anchors over all pyramid levels\n",
    "    all_anchors = np.zeros((0, 4))\n",
    "    for idx, p in enumerate(pyramid_levels):\n",
    "        anchors         = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales)\n",
    "        shifted_anchors = shift(image_shapes[idx], strides[idx], anchors)\n",
    "        all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n",
    "\n",
    "    return all_anchors\n",
    "\n",
    "\n",
    "def shift(shape, stride, anchors):\n",
    "    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n",
    "    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n",
    "\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "\n",
    "    shifts = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel(),\n",
    "        shift_x.ravel(), shift_y.ravel()\n",
    "    )).transpose()\n",
    "\n",
    "    # add A anchors (1, A, 4) to\n",
    "    # cell K shifts (K, 1, 4) to get\n",
    "    # shift anchors (K, A, 4)\n",
    "    # reshape to (K*A, 4) shifted anchors\n",
    "    A = anchors.shape[0]\n",
    "    K = shifts.shape[0]\n",
    "    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n",
    "    all_anchors = all_anchors.reshape((K * A, 4))\n",
    "\n",
    "    return all_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed2bbda9-222c-4ea1-a594-6502e74cfc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = Anchors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "efbae237-66b6-4d03-ba46-92297c935874",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(1, 3, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0769691-b4dc-468e-a785-39acce1bcfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([64, 64]), array([32, 32]), array([16, 16]), array([8, 8]), array([4, 4])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-18.6274,  -7.3137,  26.6274,  15.3137],\n",
       "         [-24.5088, -10.2544,  32.5088,  18.2544],\n",
       "         [-31.9188, -13.9594,  39.9188,  21.9594],\n",
       "         [-12.0000, -12.0000,  20.0000,  20.0000],\n",
       "         [-16.1587, -16.1587,  24.1587,  24.1587],\n",
       "         [-21.3984, -21.3984,  29.3984,  29.3984],\n",
       "         [ -7.3137, -18.6274,  15.3137,  26.6274],\n",
       "         [-10.2544, -24.5088,  18.2544,  32.5088],\n",
       "         [-13.9594, -31.9188,  21.9594,  39.9188],\n",
       "         [-10.6274,  -7.3137,  34.6274,  15.3137]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor(x)[:,:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ddc9d-2f63-42ec-9bac-c94ca21acd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[[-18.6274,  -7.3137,  26.6274,  15.3137],\n",
    "         [-12.0000, -12.0000,  20.0000,  20.0000],\n",
    "         [ -7.3137, -18.6274,  15.3137,  26.6274],\n",
    "         [-24.5088, -10.2544,  32.5088,  18.2544],\n",
    "         [-16.1587, -16.1587,  24.1587,  24.1587],\n",
    "         [-10.2544, -24.5088,  18.2544,  32.5088],\n",
    "         [-31.9188, -13.9594,  39.9188,  21.9594],\n",
    "         [-21.3984, -21.3984,  29.3984,  29.3984],\n",
    "         [-13.9594, -31.9188,  21.9594,  39.9188],\n",
    "         [-10.6274,  -7.3137,  34.6274,  15.3137]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d467482d-8224-4994-b956-909ac252a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class Anchors(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        anchor_scale: float = 4.,  # NOTE!!: anchor_scale = 4. if compound_coef != 7 else 5.\n",
    "        scales: List[float] = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)],\n",
    "        aspect_ratios: List[float] = [0.5, 1., 2.]  # width_box / height_box\n",
    "    ):\n",
    "        super(Anchors, self).__init__()\n",
    "        self.scales = scales\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.anchor_scale = anchor_scale\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, features: Tuple[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Generates multiscale anchor boxes.\n",
    "        Args:\n",
    "            inputs: Tensor (B x N x H x W): H = W = 128*compound_coef + 512\n",
    "            features: Tuple (Tensor[B x N' x H' x W']): tuple of tensors get from output of biFPN\n",
    "        Output:\n",
    "            anchors: Tensor[1 x all_anchors x 4]: all anchors of all pyramid features\n",
    "        \"\"\"\n",
    "\n",
    "        dtype, device = inputs.dtype, inputs.device\n",
    "        _, _, image_height, image_width = inputs.shape   # inputs: B x N x H x W\n",
    "\n",
    "        # stride of anchors on input size\n",
    "        features_sizes = [feature.shape[2:] for feature in features]   # List[[H_feature, W_feature]]\n",
    "        strides = [\n",
    "            (image_height // feature_height, image_width // feature_width)\n",
    "            for feature_height, feature_width in features_sizes\n",
    "        ]\n",
    "\n",
    "        anchors_over_all_pyramid_features = []\n",
    "        for stride_height, stride_width in strides:\n",
    "\n",
    "            anchors_per_pyramid_feature = []\n",
    "            for scale, ratio in itertools.product(self.scales, self.aspect_ratios):\n",
    "                if (image_width % stride_width != 0) or (image_height % stride_height != 0):\n",
    "                    raise ValueError('input size must be divided by the stride.')\n",
    "\n",
    "                # anchor base size\n",
    "                base_anchor_width = self.anchor_scale * stride_width\n",
    "                base_anchor_height = self.anchor_scale * stride_height\n",
    "\n",
    "                # anchor size\n",
    "                anchor_width = base_anchor_width * scale * math.sqrt(1 / ratio)\n",
    "                anchor_height = base_anchor_height * scale * math.sqrt(ratio)\n",
    "\n",
    "                # center of anchors\n",
    "                cx = torch.arange(\n",
    "                    start=stride_width / 2, end=image_width, step=stride_width, device=device, dtype=dtype\n",
    "                )\n",
    "                cy = torch.arange(\n",
    "                    start=stride_height / 2, end=image_height, step=stride_height, device=device, dtype=dtype\n",
    "                )\n",
    "\n",
    "                cx, cy = torch.meshgrid(cx, cy)\n",
    "                cx, cy = cx.t().reshape(-1), cy.t().reshape(-1)\n",
    "\n",
    "                # coodinates of each anchors: format anchor boxes # x1,y1,x2,y2\n",
    "                anchors = torch.stack(\n",
    "                    (\n",
    "                        cx - anchor_width / 2.,\n",
    "                        cy - anchor_height / 2.,\n",
    "                        cx + anchor_width / 2.,\n",
    "                        cy + anchor_height / 2.,\n",
    "                    ), dim=1\n",
    "                )  # num_anchors x 4\n",
    "\n",
    "                anchors = anchors.unsqueeze(dim=1)  # num_anchors x 1 x 4\n",
    "                anchors_per_pyramid_feature.append(anchors)\n",
    "\n",
    "            # num_anchors x (scale * aspect_ratios) x 4\n",
    "            anchors_per_pyramid_feature = torch.cat(anchors_per_pyramid_feature, dim=1)\n",
    "            # (num_anchors * scale * aspect_ratios) x 4\n",
    "            anchors_per_pyramid_feature = anchors_per_pyramid_feature.reshape(-1, 4)\n",
    "            anchors_over_all_pyramid_features.append(anchors_per_pyramid_feature)\n",
    "\n",
    "        # [(num_anchors * scale * aspect_ratios) * pyramid_levels] x 4\n",
    "        anchors = torch.vstack(anchors_over_all_pyramid_features)\n",
    "\n",
    "        return anchors.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "afe063a8-1d19-409e-a3bb-5e6038beaf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = Anchors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7bebba1-7646-44ba-abc8-2c745ebba817",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.FloatTensor(1, 3, 512, 512)\n",
    "features = [\n",
    "    torch.FloatTensor(1, 256, 64, 64),\n",
    "    torch.FloatTensor(1, 256, 32, 32),\n",
    "    torch.FloatTensor(1, 256, 16, 16),\n",
    "    torch.FloatTensor(1, 256, 8, 8),\n",
    "    torch.FloatTensor(1, 256, 4, 4),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0e1ccb25-7555-4e70-9a89-ce5f1e13b05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49104, 4])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor(inputs, features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3f625be0-60b8-4df5-a2be-1747c513acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.FloatTensor(5)\n",
    "b = torch.FloatTensor(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c7ac8d25-3b03-43fd-a4b9-2f92585f4874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.stack((a, b)).t()\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f0adc4b3-abfd-47d9-9952-3fce9a6b06c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "tensor([[5.9969e-22, 4.5785e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41,\n",
      "         4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41],\n",
      "        [4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41,\n",
      "         4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41],\n",
      "        [4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41,\n",
      "         4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41],\n",
      "        [4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41,\n",
      "         4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41],\n",
      "        [4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41, 4.5783e-41,\n",
      "         4.5783e-41, 4.5783e-41, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor(5, 10, 3)\n",
    "scores, _ = a.max(dim=2)\n",
    "print(scores.shape)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3610c493-d0fc-4061-af99-1b9787eb94f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False]])\n"
     ]
    }
   ],
   "source": [
    "batch_scores_over_threshold = (scores > 1e-50)\n",
    "print(batch_scores_over_threshold.shape)\n",
    "print(batch_scores_over_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48e2bf5f-b6c6-49e6-949f-9553cb08486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True, False, False])\n"
     ]
    }
   ],
   "source": [
    "sample_scores_over_threshold = batch_scores_over_threshold[4, :]\n",
    "print(sample_scores_over_threshold.shape)\n",
    "print(sample_scores_over_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bc2a5257-f297-4bbd-bbb3-69e7803ffe6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0862e-27,  4.5783e-41, -1.3418e-27],\n",
       "        [ 4.5783e-41, -1.3416e-27,  4.5783e-41],\n",
       "        [-1.3415e-27,  4.5783e-41, -1.3418e-27],\n",
       "        [ 4.5783e-41, -1.3417e-27,  4.5783e-41],\n",
       "        [-1.3415e-27,  4.5783e-41, -1.3417e-27],\n",
       "        [ 4.5783e-41, -1.3415e-27,  4.5783e-41],\n",
       "        [-1.3417e-27,  4.5783e-41, -1.3416e-27],\n",
       "        [ 4.5783e-41, -1.3416e-27,  4.5783e-41]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1, sample_scores_over_threshold, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2939b669-16f3-457b-a3db-dc6d33a486d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1, sample_scores_over_threshold, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209af791-9683-4ed1-9aa8-12dc76436941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
